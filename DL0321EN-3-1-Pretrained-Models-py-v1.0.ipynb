{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5044e6d44248dc8528b1969f13f336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/261482368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6edb2a271142b981a485f7aa975e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 00:50:55.324775: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2023-11-08 00:50:55.346056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394310000 Hz\n",
      "2023-11-08 00:50:55.346719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559e1e52a860 executing computations on platform Host. Devices:\n",
      "2023-11-08 00:50:55.346763: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2023-11-08 00:50:55.445844: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fa1fa1d7250>,\n",
       " <keras.layers.core.Dense at 0x7fa1f1cc8c90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fa285d35c10>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fa2817f9ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2817f9790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa28191abd0>,\n",
       " <keras.layers.core.Activation at 0x7fa28191aed0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fa28005ad10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa280053a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa281467110>,\n",
       " <keras.layers.core.Activation at 0x7fa281467cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa28147c550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa27416cad0>,\n",
       " <keras.layers.core.Activation at 0x7fa274137bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa274087d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa254743c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa254787fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa254770110>,\n",
       " <keras.layers.merge.Add at 0x7fa254770d90>,\n",
       " <keras.layers.core.Activation at 0x7fa2546a3a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2545c51d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2546320d0>,\n",
       " <keras.layers.core.Activation at 0x7fa25461d690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2545b7210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa25452e090>,\n",
       " <keras.layers.core.Activation at 0x7fa25452e250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa25444c350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2543e99d0>,\n",
       " <keras.layers.merge.Add at 0x7fa254430a50>,\n",
       " <keras.layers.core.Activation at 0x7fa254366bd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa254316810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2542c5b10>,\n",
       " <keras.layers.core.Activation at 0x7fa2542c53d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa254262ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2541c4b50>,\n",
       " <keras.layers.core.Activation at 0x7fa2541c4f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2540f90d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa254095dd0>,\n",
       " <keras.layers.merge.Add at 0x7fa2540daa10>,\n",
       " <keras.layers.core.Activation at 0x7fa23c7bbc50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c7586d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c748250>,\n",
       " <keras.layers.core.Activation at 0x7fa23c7b1110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c6cbbd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c6b5f50>,\n",
       " <keras.layers.core.Activation at 0x7fa23c6b5ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c5e2550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c47e950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c4fe750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c497d90>,\n",
       " <keras.layers.merge.Add at 0x7fa23c3e1350>,\n",
       " <keras.layers.core.Activation at 0x7fa23c3f7f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c2c0ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c374590>,\n",
       " <keras.layers.core.Activation at 0x7fa23c374f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c296dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c276790>,\n",
       " <keras.layers.core.Activation at 0x7fa23c276510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c190d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa23c0f9a50>,\n",
       " <keras.layers.merge.Add at 0x7fa23c0f9e50>,\n",
       " <keras.layers.core.Activation at 0x7fa23c0a7150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa23c0478d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2187cc410>,\n",
       " <keras.layers.core.Activation at 0x7fa2187dfed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa218702f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2186e3690>,\n",
       " <keras.layers.core.Activation at 0x7fa2186e3790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2185fa790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2185eaf10>,\n",
       " <keras.layers.merge.Add at 0x7fa2185eadd0>,\n",
       " <keras.layers.core.Activation at 0x7fa218515290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2184a55d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa218479390>,\n",
       " <keras.layers.core.Activation at 0x7fa21848fe90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa21842ebd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2183b7e10>,\n",
       " <keras.layers.core.Activation at 0x7fa21838f4d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa218326f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa218293fd0>,\n",
       " <keras.layers.merge.Add at 0x7fa2182a7a90>,\n",
       " <keras.layers.core.Activation at 0x7fa2181c30d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2181c37d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2181a3950>,\n",
       " <keras.layers.core.Activation at 0x7fa2181a3910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa2180daa90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa2180b1450>,\n",
       " <keras.layers.core.Activation at 0x7fa2180b1fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fbf98c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fbeb6410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fbf05ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fbe1d850>,\n",
       " <keras.layers.merge.Add at 0x7fa1fbdd0ed0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fbdb9a10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fbd6b650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fbd34f50>,\n",
       " <keras.layers.core.Activation at 0x7fa1fbd34490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fbc67290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fbbc7f10>,\n",
       " <keras.layers.core.Activation at 0x7fa1fbbc7cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fbb66dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fbade210>,\n",
       " <keras.layers.merge.Add at 0x7fa1fbade2d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fba78050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb942990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb9f1f90>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb9f1d90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb910790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb8fbf10>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb8fbdd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb829190>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb78ba50>,\n",
       " <keras.layers.merge.Add at 0x7fa1fb78bad0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb729c10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb6dd210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb6a1410>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb6a1ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb5dbf90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb5b96d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb5b97d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb4d1dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb4bda10>,\n",
       " <keras.layers.merge.Add at 0x7fa1fb4bde10>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb3ea210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb5db450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb34d8d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb362ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb2ff9d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb279e90>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb21ded0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb197bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb135390>,\n",
       " <keras.layers.merge.Add at 0x7fa1fb17ef90>,\n",
       " <keras.layers.core.Activation at 0x7fa1fb093e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fb04d150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fb02b0d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fafedf50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1faf4aa90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1faf1b490>,\n",
       " <keras.layers.core.Activation at 0x7fa1faf1b750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fae42890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fae3bf50>,\n",
       " <keras.layers.merge.Add at 0x7fa1fadd7250>,\n",
       " <keras.layers.core.Activation at 0x7fa1fad5ac50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1face7510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1facc1cd0>,\n",
       " <keras.layers.core.Activation at 0x7fa1facc1110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fac73150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fabd5b10>,\n",
       " <keras.layers.core.Activation at 0x7fa1fabd5450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fab72f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1faa29e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1faaea450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa9c01d0>,\n",
       " <keras.layers.merge.Add at 0x7fa1fa9053d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa91dfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fa8bb690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa89a510>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa89afd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fa839110>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa7b11d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa91d250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fa6cdd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa6fe490>,\n",
       " <keras.layers.merge.Add at 0x7fa1fa6fead0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa5caed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fa5caf90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa546990>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa5468d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fa48af10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa45d8d0>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa45d650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fa1fa3f9650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fa1fa363ed0>,\n",
       " <keras.layers.merge.Add at 0x7fa1fa363f90>,\n",
       " <keras.layers.core.Activation at 0x7fa1fa292d50>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fa1fa5ca590>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fa280053e10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      "  5/301 [..............................] - ETA: 6:00:59 - loss: 0.6329 - acc: 0.6280"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Anurag Rana.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2023-11-08  | 2.0  | Anurag Rana  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
